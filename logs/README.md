### Naming conventions

These log files are generated by the rank 0 pod in a distributed PyTorch job. We started with 2 nodes and eventually expanded to 8 nodes. Each node has 4 H100s.

*profile1* means the jobs were run with `nsys profile` on and *profile0* means jobs were run without `nsys profile`. TODO but low priority: run all remaining jobs i.e. fill grad of profile 1 and 0. Profile files are generally big and haven't been pushed to the repository.

grad_accum refers to gradient accumulation steps i.e. number of micro-batches in a batch or number of forward/backward passes before an all-reduce is called and before the model and optimizer are updated. We scan grad_accum from 1 through 10.

We mainly use the log files to extract time taken per iteration. `scripts/gen_csv.sh` is used to extract the times into a CSV file which is stored in `data/`.

##### Scaling pods with 4 GPUs each

Goal: measuring overheads induced as more nodes (with 1 pod each) were added.

**logs_profile1_modeld12_npods1_nprocs4**: 1 pod with 4 GPUs. GPU-GPU connections are NVLink. The pod occupies the full node.

**logs_profile1_modeld12_npods2_nprocs4**: 2 pods with 4 GPUs each. This requires using two nodes with 1 pod/node. Distributed Data Parallel (DDP) brings node-node network connections into play.

**logs_profile0_modeld12_npods4_nprocs4**: 4 pods (and therefore 4 nodes) with 4 GPUs each.

**logs_profile0_modeld12_npods8_nprocs4**: 8 pods (and therefore 8 nodes) with 4 GPUs each.


##### Difference between two different nodes (when we only had two nodes)

Goal: to ensure performance on two nodes was statistically identical.

**logs_profile1_modeld12_npods2_nprocs2_wrk5_run3**: 2 pods with 2 GPUs each on node "wrk5"

**logs_profile1_modeld12_npods2_nprocs2_wrk6_run3a**: 2 pods with 2 GPUs each on node "wrk6"

##### Different number of pods on two nodes

Goal: to measure overheads induced by additional pods. Additional experiments and tracing will decompose the various effects contributing to overheads.

**logs_profile1_modeld12_npods2_nprocs4** (also listed above): 2 pods with 4 GPUs each on two nodes.

**logs_profile1_modeld12_npods4_nprocs2**: 4 pods with 2 GPUs each on two nodes.

**logs_profile1_modeld12_npods8_nprocs1** and **logs_profile1_modeld12_npods8_nprocs1_run2**: Two separate runs with 8 pods with 1 GPU each. We only had two nodes when these experiments ran so all GPUs on the two nodes were used.


