MODEL    : d12
BS        : 32
SEQ LEN   : 1024
GRAD ACCUM: 1
BS TOTAL  : 262144
LOG FILE: /tmp/logs/stdout_npods2_nprocs4_numiter10_gradaccum1_bs32_seq1024_totalbs262144_profile1_modeld12_typeTCP_runidmay21protoscanhostnetworkncclfix.log
NODE RANK: 0
PROFILE  : 1
TAG      : npods2_nprocs4_numiter10_gradaccum1_bs32_seq1024_totalbs262144_profile1_modeld12_typeTCP_runidmay21protoscanhostnetworkncclfix
RUNID    : may21protoscanhostnetworkncclfix
--------------------------
NV_LIBCUBLAS_VERSION=12.0.2.224-1
KUBERNETES_SERVICE_PORT_HTTPS=443
NVIDIA_VISIBLE_DEVICES=GPU-5664f65c-e52c-5abc-ac5d-ed77b7f2d29f,GPU-b11198d9-1e10-0819-aa42-f33611f9d81c,GPU-1e0f42d7-f3fa-db2c-6ef1-de34707cc1e6,GPU-16a0c4f7-9c5f-fc98-9da8-192bb0557f26
KUBERNETES_SERVICE_PORT=443
HOSTNAME=mocr4pcc02u16
NSS_SDB_USE_CACHE=no
NVIDIA_REQUIRE_CUDA=cuda>=12.0 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471
NV_NVTX_VERSION=12.0.140-1
POD_NAME=torchrun-multipod-0
NV_LIBCUSPARSE_VERSION=12.0.1.140-1
NV_LIBNPP_VERSION=12.0.1.104-1
NCCL_VERSION=2.16.5-1
NCCL_SOCKET_IFNAME=eno6np0
NCCL_DEBUG_SUBSYS=ALL
PWD=/workspace
NVIDIA_DRIVER_CAPABILITIES=compute,utility
NV_LIBNPP_PACKAGE=libnpp-12-0=12.0.1.104-1
NCCL_DEBUG=INFO
NCCL_IB_OOB_IPV6_DISABLE=1
NVIDIA_PRODUCT_NAME=CUDA
NV_CUDA_CUDART_VERSION=12.0.146-1
HOME=/root
NCCL_NSOCKS_PERTHREAD=4
KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443
CUDA_VERSION=12.0.1
NV_LIBCUBLAS_PACKAGE=libcublas-12-0=12.0.2.224-1
NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-0
NCCL_DEBUG_FILE=/tmp/logs/nccl_debug
TERM=xterm
TORCH_NCCL_ASYNC_ERROR_HANDLING=1
SHLVL=0
NV_CUDA_LIB_VERSION=12.0.1-1
NVARCH=x86_64
NCCL_SOCKET_NTHREADS=16
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1
NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-0
POD_IP=192.168.50.145
NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
NCCL_IB_DISABLE=1
KUBERNETES_SERVICE_HOST=172.30.0.1
KUBERNETES_PORT=tcp://172.30.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
NV_LIBNCCL_PACKAGE_NAME=libnccl2
NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
_=/usr/bin/env
--------------------------
Dumping initial: /proc/net/dev
Inter-|   Receive                                                |  Transmit
 face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed
    lo: 18906744543 18903078    0    0    0     0          0         0 18906744543 18903078    0    0    0     0       0          0
enp8s0f4u1u6: 63863068  576826    0    0    0     0          0         0 39958720  311836    0    0    0     0       0          0
  eno4:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
ovs-system:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
genev_sys_6081: 38343267696 2756387    0    0    0     0          0         0 42083868419 2641388    0  185    0     0       0          0
ovn-k8s-mp0: 360771141 3260137    0    0    0     0          0         0 1620442359 3612731    0    0    0     0       0          0
br-int:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
 br-ex: 79982836608 32354958    0    0    0     0          0         0 84288480826 32100986    0    0    0     0       0          0
38517feb485c9e3: 159252187  514356    0    0    0     0          0         0 905664998  578647    0    0    0     0       0          0
bfbafd3156cfa64: 22445104  277824    0    0    0     0          0         0 37162162  530175    0    0    0     0       0          0
3626971effa20e3: 38466883  369592    0    0    0     0          0         0 38601152  441976    0    0    0     0       0          0
f7a0c0d06570eeb:  132028     783    0    0    0     0          0         0  1077275     699    0    0    0     0       0          0
1e7f453a6c90e3d: 141831180  443441    0    0    0     0          0         0 84424552  392611    0    0    0     0       0          0
8dd94b13211c29f: 343767590 1781962    0    0    0     0          0         0 219177843 2053318    0    0    0     0       0          0
62d3251ef2510e4:   10524     185    0    0    0     0          0         0    12946     183    0    0    0     0       0          0
902e8fa8e55bb23:   15797     211    0    0    0     0          0         0    29806     199    0    0    0     0       0          0
e22fd0daac6232e: 1595136391  387105    0    0    0     0          0         0 1479490882  672541    0    0    0     0       0          0
1862597e508722c: 2659710775  896487    0    0    0     0          0         0 1648304165 1000576    0    0    0     0       0          0
e5a934737ea561c:   10434     184    0    0    0     0          0         0    12856     182    0    0    0     0       0          0
b143492a06b8b70: 7335258   92092    0    0    0     0          0         0 117500372   77094    0    0    0     0       0          0
dfec25a8124aac9:   29164     285    0    0    0     0          0         0   136756     265    0    0    0     0       0          0
50f7d2bd0eb527e: 135747872  857014    0    0    0     0          0         0 211956927  954959    0    0    0     0       0          0
eno2np0:      86       1    0    0    0     0          0         1  9134048   54208    0    0    0     0       0          0
eno3np1:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
eno6np0: 46598046457 6632612    0    0    0     0          0     64670 46568353340 6496631    0    0    0     0       0          0
eno5np0:       0       0    0    0    0     0          0         0 11688450   69368    0    0    0     0       0          0
eno8np0:       0       0    0    0    0     0          0         0 11698942   69490    0    0    0     0       0          0
eno7np0: 81930005031 76701904    0 2687    0     0          0    589223 86656806524 84773385    0    0    0     0       0          0
eno6v0:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
eno6v1:   18177      96    0    0    0     0          0        47     5500      75    0    0    0     0       0          0
eno6v2:   12733      79    0    0    0     0          0        29     4548      63    0    0    0     0       0          0
eno6v3:   37361     113    0    0    0     0          0       113     2024      26    0    0    0     0       0          0
eno6v4:   29776     129    0    0    0     0          0        80     4628      64    0    0    0     0       0          0
eno6v5:  137203    1371    0    0    0     0          0       155   473122    1496    0    0    0     0       0          0
eno6v6:   92981     397    0    0    0     0          0        20    25266     345    0    0    0     0       0          0
eno6v7:    3687      11    0    0    0     0          0        11      942      12    0    0    0     0       0          0
W0522 00:14:02.581000 84 torch/distributed/run.py:766] 
W0522 00:14:02.581000 84 torch/distributed/run.py:766] *****************************************
W0522 00:14:02.581000 84 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0522 00:14:02.581000 84 torch/distributed/run.py:766] *****************************************
Running pytorch 2.7.0+cu126
using device: cuda:0
total desired batch size: 262144
=> calculated gradient accumulation steps: 1
using device: cuda:1
using device: cuda:3
using device: cuda:2
compiling the model...
DataLoader: total number of tokens: 100,000,000 across 1 files
num decayed parameter tensors: 50, with 124,318,464 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: False
using regular AdamW
step    1/10 | train loss 11.008541 | norm 15.8513 | lr 1.00e-04 | (172136.12 ms | 1523 tok/s)
step    2/10 | train loss 10.116471 | norm 5.8736 | lr 1.00e-04 | (208.54 ms | 1257033 tok/s)
step    3/10 | train loss 9.765995 | norm 2.6306 | lr 1.00e-04 | (192.34 ms | 1362885 tok/s)
step    4/10 | train loss 9.596249 | norm 2.1068 | lr 1.00e-04 | (183.67 ms | 1427272 tok/s)
step    5/10 | train loss 9.513400 | norm 2.1000 | lr 1.00e-04 | (182.24 ms | 1438449 tok/s)
step    6/10 | train loss 9.431214 | norm 1.9935 | lr 1.00e-04 | (187.02 ms | 1401653 tok/s)
step    7/10 | train loss 9.345900 | norm 1.9369 | lr 1.00e-04 | (190.11 ms | 1378871 tok/s)
step    8/10 | train loss 9.256424 | norm 1.9010 | lr 1.00e-04 | (195.95 ms | 1337817 tok/s)
step    9/10 | train loss 9.164138 | norm 1.8592 | lr 1.00e-04 | (165.36 ms | 1585330 tok/s)
step   10/10 | train loss 9.071947 | norm 1.7839 | lr 1.00e-04 | (168.42 ms | 1556520 tok/s)
final 9 iters avg: 185.962ms
peak memory consumption: 16118 MiB
